# 神经网络

### 1. 感知机：

$$
y=\sum x_i\times w_i + b
$$

​	采用奖惩机制，如果输出结果符合，则提高权重比列，否则反之，	直至某个权重对所有样本均不产生错误。

​	
$$
w_i=w_i+\eta\times\left(d-y_i\right)\times x_i
\\ 调整算法
$$
为了量化误差，将总误差表示为权重的函数，即损失函数：$J(W)$

所以，为使误差最小，需要损失函数能得到最小值，即损失函数可导，由于损失函数与激活函数相关，所以损失函数可导即意味着激活函数可导，其中一种激活函数为sigmoid函数
$$
\text{S}\left ( z \right )=\frac {1}{1+e^{-x}}
\\ \text{sigmoid 函数}
$$

#### 标准神经元模型特征：

1. 多个输入端以及其对应的权重
2. 有类似sigmoid的激活函数
3. 通过调整算法，利用梯度下调法调整权重
4. 具有多个输出端

### 2.  前馈神经网络 ——Feedforward Neural Network（FNN）

 组成：

1. 1层输入层
2. 1层或多层隐含层
3. 1层输出层

<img width="754" height="426" alt="image" src="https://github.com/user-attachments/assets/f726a2e3-271a-423b-9418-493228fddfb4" />


​			全连接：后层每一个神经元输入为前一层所有神经元的输出加权重（即激活值）

前馈计算：将输入层的激活值传输到输出层

#### BP算法

原理：通过绝对误差值$E$反推权重调整量。

<img width="1463" height="667" alt="image" src="https://github.com/user-attachments/assets/eab2ed58-1e3e-4df6-8556-a8b22b2c3ba9" />


#### FNN神经网络训练过程

1. 前馈计算
2. 误差计算
3. 误差反传
4. 权重更新

##### FNN神经网络模型的激活函数

由于神经元输入更会偏向0或1的饱和区域，导致Sigmoid函数的导数值偏向0，使误差信号不能传到下一层。因此，采用ReLU作为激活函数。
$$
y=\left\{\begin{matrix}
0 \quad if\quad x<0
 \\ 
x \quad if\quad x \ge 0 
\end{matrix}\right.{}
\\
ReLU函数
$$

#### 3. CNN 卷积神经网络

1. 结构：

   - 输入层（2维矩阵）
   - $M\times N$层卷积层$\to $ ReLU
   - $b\times N$层池化层
   - k层全连接层
   - softmax 输出层

 2. 卷积层

    与输入层相连的卷积层所有卷积核需要处理所有的输入；

    一层卷积层中含有多个卷积核，用于独立处理全部或部分输入；

 3. 池化层

    由于多层卷积导致的信息冗余，池化层采用maxpooling操作，以此保留重要信息

    <img width="893" height="276" alt="image" src="https://github.com/user-attachments/assets/a57b8fe1-23a1-4158-9be3-e09ddaeded4c" />


 4. 全连接层

    最后一个卷积层将所有卷积核处理的信息拉平成一维向量，经过多个FFNN层进入到输出层





